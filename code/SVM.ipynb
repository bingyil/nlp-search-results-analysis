{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition, pipeline, metrics\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Metric (Quadratic Weighted Kappa)\n",
    "# The following 3 functions are taken from https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>median_relevance</th>\n",
       "      <th>relevance_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bridal shower decoration</td>\n",
       "      <td>accent pillow heart design red black</td>\n",
       "      <td>red satin accent pillow embroider heart black ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lead christmas light</td>\n",
       "      <td>set battery operate multi lead train christmas...</td>\n",
       "      <td>set battery operate train christmas light item...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>projector</td>\n",
       "      <td>viewsonic pro dlp multimedia projector</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wine rack</td>\n",
       "      <td>concept housewares wr solid wood ceiling wall ...</td>\n",
       "      <td>like silent sturdy tree southern enterprise bi...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>light bulb</td>\n",
       "      <td>wintergreen light christmas lead light bulb pack</td>\n",
       "      <td>wtgr feature nickel base average hour acrylic ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      query  \\\n",
       "0  bridal shower decoration   \n",
       "1      lead christmas light   \n",
       "2                 projector   \n",
       "3                 wine rack   \n",
       "4                light bulb   \n",
       "\n",
       "                                       product_title  \\\n",
       "0               accent pillow heart design red black   \n",
       "1  set battery operate multi lead train christmas...   \n",
       "2             viewsonic pro dlp multimedia projector   \n",
       "3  concept housewares wr solid wood ceiling wall ...   \n",
       "4   wintergreen light christmas lead light bulb pack   \n",
       "\n",
       "                                 product_description  median_relevance  \\\n",
       "0  red satin accent pillow embroider heart black ...                 1   \n",
       "1  set battery operate train christmas light item...                 4   \n",
       "2                                                                    4   \n",
       "3  like silent sturdy tree southern enterprise bi...                 4   \n",
       "4  wtgr feature nickel base average hour acrylic ...                 2   \n",
       "\n",
       "   relevance_variance  \n",
       "0               0.000  \n",
       "1               0.000  \n",
       "2               0.471  \n",
       "3               0.000  \n",
       "4               0.471  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_clean = pd.read_csv(\"./train_clean.csv\")\n",
    "train_clean = train_clean.fillna(\"\")\n",
    "train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each observation, combine **query**, **product_title** and **product_description** to create a giant text. By doing so we can capture the similiaritis between these columns. For example, if a input query is \"lead christmas light\", and both product_title and product_description contain the key words \"lead\", \"christmas\" and \"light\", then the vectorized text will be longer in these three dimensions and shorter in others. If the input query share no common word with product_title or product_description, the vectorized text will have the same length in all the dimensions. <br/>\n",
    "<br/>\n",
    "However, there is one potential problem in this method. If the input query is totally different from product_title and product_description, but product_title and product_description are similar, we will still get a feature vector seeming to have high relevance. So to avoid such issue, it would be better to combine one column with the input query. Since there are much more missing values in product_description, I choose to combine \"query\" and \"product_title\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = train_clean[\"query\"] + [\" \"] + train_clean[\"product_title\"]\n",
    "traindata = tmp.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels\n",
    "y = train_clean[\"median_relevance\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clean = pd.read_csv(\"./test_clean.csv\")\n",
    "test_clean = test_clean.fillna(\"\")\n",
    "tmp = test_clean[\"query\"] + [\" \"] + test_clean[\"product_title\"]\n",
    "testdata = tmp.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer = \"word\", \\\n",
    "                      tokenizer = None, \\\n",
    "                      preprocessor = None, \\\n",
    "                      stop_words = None, \\\n",
    "                      max_features = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv.fit(traindata)\n",
    "X =  ctv.transform(traindata) \n",
    "X_test = ctv.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). \n",
    "svd = TruncatedSVD()\n",
    "# Standardize\n",
    "scl = StandardScaler()\n",
    "# Use SVM\n",
    "svm_model = SVC()\n",
    "\n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('svm', svm_model)])\n",
    "# Tuning hyper-parameter with grid search\n",
    "param_grid = {'svd__n_components' : [200, 400],\n",
    "              'svm__C': [10, 12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] svd__n_components=200, svm__C=10 ................................\n",
      "[CV] svd__n_components=200, svm__C=10 ................................\n",
      "[CV] svd__n_components=200, svm__C=12 ................................\n",
      "[CV] svd__n_components=200, svm__C=12 ................................\n",
      "[CV] svd__n_components=400, svm__C=10 ................................\n",
      "[CV] svd__n_components=400, svm__C=10 ................................\n",
      "[CV] svd__n_components=400, svm__C=12 ................................\n",
      "[CV] svd__n_components=400, svm__C=12 ................................\n",
      "[CV]  svd__n_components=200, svm__C=10, score=0.582924458254033, total=  40.7s\n",
      "[CV]  svd__n_components=200, svm__C=12, score=0.5848008153947173, total=  40.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   52.0s remaining:  2.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=200, svm__C=10, score=0.5483821209704955, total=  41.6s\n",
      "[CV]  svd__n_components=200, svm__C=12, score=0.550293458448539, total=  41.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:   53.0s remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   8 | elapsed:   53.0s remaining:   53.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=400, svm__C=10, score=0.5340066732457942, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   8 | elapsed:  1.5min remaining:   54.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=400, svm__C=12, score=0.5407554744487292, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   8 | elapsed:  1.5min remaining:   30.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=400, svm__C=10, score=0.546330497938208, total= 1.3min\n",
      "[CV]  svd__n_components=400, svm__C=12, score=0.5350237903158717, total= 1.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5675\n",
      "Best parameters set:\n",
      "\tsvd__n_components: 200\n",
      "\tsvm__C: 12\n"
     ]
    }
   ],
   "source": [
    "kappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better=True)\n",
    "\n",
    "model = GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n",
    "                     verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "model.fit(X, y)\n",
    "print(\"Best score: %0.4f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Get best model\n",
    "best_model = model.best_estimator_\n",
    "\n",
    "# Fit model with best parameters optimized for quadratic_weighted_kappa\n",
    "best_model.fit(X,y)\n",
    "preds = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "idx = test[\"id\"].values\n",
    "submission = pd.DataFrame({\"id\": idx, \"prediction\": preds})\n",
    "submission.to_csv(\"CTV_SVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private leader board score 0.60345."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
    "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 5), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "\n",
    "tfv.fit(traindata)\n",
    "X =  tfv.transform(traindata) \n",
    "X_test = tfv.transform(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). \n",
    "svd = TruncatedSVD()\n",
    "# Standardize\n",
    "scl = StandardScaler()\n",
    "# Use SVM\n",
    "svm_model = SVC()\n",
    "\n",
    "clf = pipeline.Pipeline([('svd', svd),\n",
    "                         ('scl', scl),\n",
    "                         ('svm', svm_model)])\n",
    "# Tuning hyper-parameter with grid search\n",
    "param_grid = {'svd__n_components' : [200, 400],\n",
    "              'svm__C': [10, 12]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] svd__n_components=200, svm__C=10 ................................\n",
      "[CV] svd__n_components=200, svm__C=10 ................................\n",
      "[CV] svd__n_components=200, svm__C=12 ................................\n",
      "[CV] svd__n_components=200, svm__C=12 ................................\n",
      "[CV] svd__n_components=400, svm__C=10 ................................\n",
      "[CV] svd__n_components=400, svm__C=10 ................................\n",
      "[CV] svd__n_components=400, svm__C=12 ................................\n",
      "[CV] svd__n_components=400, svm__C=12 ................................\n",
      "[CV]  svd__n_components=200, svm__C=10, score=0.5132496473226699, total=  44.1s\n",
      "[CV]  svd__n_components=200, svm__C=12, score=0.522284488387325, total=  44.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   8 | elapsed:   53.5s remaining:  2.7min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=200, svm__C=10, score=0.5177033785906953, total=  44.9s\n",
      "[CV]  svd__n_components=200, svm__C=12, score=0.5238631720572573, total=  45.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   8 | elapsed:   54.1s remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   8 | elapsed:   54.3s remaining:   54.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=400, svm__C=10, score=0.5349037749231415, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   8 | elapsed:  1.3min remaining:   47.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=400, svm__C=10, score=0.5624675916806688, total= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   8 | elapsed:  1.3min remaining:   26.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  svd__n_components=400, svm__C=12, score=0.5407940266777758, total= 1.2min\n",
      "[CV]  svd__n_components=400, svm__C=12, score=0.5664372746028488, total= 1.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  1.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.5536\n",
      "Best parameters set:\n",
      "\tsvd__n_components: 400\n",
      "\tsvm__C: 12\n"
     ]
    }
   ],
   "source": [
    "# Kappa Scorer \n",
    "kappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better = True)\n",
    "\n",
    "# Initialize Grid Search Model\n",
    "model = GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n",
    "                     verbose=10, n_jobs=-1, iid=True, refit=True, cv=2)\n",
    "\n",
    "# Fit Grid Search Model\n",
    "model.fit(X, y)\n",
    "print(\"Best score: %0.4f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Get best model\n",
    "best_model = model.best_estimator_\n",
    "\n",
    "# Fit model with best parameters optimized for quadratic_weighted_kappa\n",
    "best_model.fit(X,y)\n",
    "preds = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "submission = pd.DataFrame({\"id\": idx, \"prediction\": preds})\n",
    "submission.to_csv(\"TFIDF_SVM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Private leader board score 0.59078."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
